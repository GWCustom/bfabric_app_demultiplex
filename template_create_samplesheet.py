"""
The purpose of this file is to create the data required to run the Nextflow pipeline for NovaSeq6000: ILMN_mRNA_Ligation_Prep.
Three files are required:

- Samplesheet.csv
- pipeline_samplesheet.csv
- NFC_DMX.config

The Samplesheet.csv and pipeline_samplesheet.csv files are generated by a function,
while the NFC_DMX.config file must be created manually by the user.
"""

# to do 
# 1. add container_id to the samples
# 2. take the base_path from the api call and to not hardcode it!

import sys
sys.path.append("../bfabric-web-apps")

from sample_sheet import SampleSheet, Sample
import bfabric_web_apps
from bfabric_web_apps.objects.BfabricInterface import bfabric_interface
import csv
from datetime import datetime

url_params = "token=2q4lKuG9HNJr4bK7W0c3sGeF7xg0x5pdIqnAvnPCmSerzHtHyLheS8yKPnSSP1zmahE5KabOIMuhgl8iL0Yv9nBCbPPvaiXlH35X8H7_t5Hrg0xuELHmI68WgybFQfAfa_erfPUMAUi6-PCdCmgPsYLkr2ilznD-pTDNj656wWBF8YbHbnt0qjxatyjJhBkFnz_C2NTyhjZDCAfI4UVwbfgAAs3CU94Oly-wmNiVx4hBxR12aN1VyF1XqzRzuMxb"
token, token_data, entity_data, app_data, page_title, session_details, job_link = bfabric_web_apps.process_url_and_token(url_params)

print("Token data:", token_data)
print("Entity data:", entity_data)
print("App data:", app_data)
print("Page title:", page_title)
print("Session details:", session_details)
print("Job link:", job_link)

# Create Samplesheet.csv

L = bfabric_web_apps.get_logger(token_data)
wrapper = bfabric_interface.get_wrapper()

samples = L.logthis(
    api_call=wrapper.read,
    endpoint="sample",
    obj={"runid": token_data['entity_id_data']},
    params=None,
    flush_logs=False
)

run = L.logthis(
    api_call=wrapper.read,
    endpoint="run",
    obj={"id": 1913},
    flush_logs=False
)

rununit = L.logthis(
    api_call=wrapper.read,
    endpoint="rununit",
    obj={"runid": token_data['entity_id_data']},
    flush_logs=False
)

instrument_id = rununit[0]['instrument']['id']

instrument_data = L.logthis(
    api_call=wrapper.read,
    endpoint="instrument",
    obj={"id": instrument_id},
    flush_logs=False
)

workflow = L.logthis(
    api_call=wrapper.read,
    endpoint="workflow",
    obj={"id": 1619},
    flush_logs=False
)

# api calles don't need to be logged one by one!


print("samples",samples)
print("--------------------------------------")
print("run",run)
print("--------------------------------------")
print("rununit",rununit)
print("--------------------------------------")
print("instrument_data",instrument_data)
print("--------------------------------------")
print("workflow",workflow)



def create_samplesheet(app_data, samples, rununit, instrument_data, output_file="Samplesheet.csv"):
    """
    Create a NovaSeq6000 sample sheet (mRNA ligation prep) with format:
    """
    rununit_data = rununit[0]
    instrument_data = instrument_data[0]
    # 1. Create a fresh SampleSheet object
    ss = SampleSheet()

    # 2. Populate the [Header] section (use direct assignment instead of add_attr)
    ss.Header["IEMFileVersion"] = 5 #Hardcode
    ss.Header["Experiment Name"] = rununit_data.get("name")
    ss.Header["Date"] = manipulate_date_format(rununit_data.get("created"))
    ss.Header["Workflow"] = "GenerateFASTQ" #Hardcode
    ss.Header["Application"] = app_data.get("name")
    ss.Header["Instrument Type"] = instrument_data.get("name")
    #ss.Header["Assay"] = "Nextera Flex for Enrichment"
    #ss.Header["Index Adapters"] = "IDT-ILMN Nextera DNA UD Indexes (384 Indexes)"

    # 3. Populate the [Reads] section
    ss.Reads = [76, 76] #Hardcode

    # 4. Populate the [Settings] section
    ss.Settings["Adapter"] = "CTGTCTCTTATACACATCT" #Hardcode

    # 5. Create rows in the [Data] section
    #
    #    Important: We store i7 in 'index' and i5 in 'index2' to avoid duplication errors.
    for record in samples:
        sample_dict = {
            "Sample_ID":         record["id"],
            "Sample_Name":       record["name"],
            "Sample_Plate":      "",
            "Sample_Well":       "",
            "Index_Plate":       "",
            "Index_Plate_Well":  "",
            "I7_Index_ID":       record["multiplexiddmx"],  # purely for final CSV documentation
            "index":             record["multiplexiddmx"],  # i7 for demultiplexing
            "I5_Index_ID":       record["multiplexid2dmx"], # purely for final CSV documentation
            "index2":            record["multiplexid2dmx"], # i5 for demultiplexing
            "Sample_Project":    "", #container_id
            "Description":       ""
        }

        ss.add_sample(Sample(sample_dict))

    # 6. Write to CSV
    with open(output_file, "w", newline="") as handle:
        ss.write(handle)

    print(f"Sample sheet written to {output_file}")



def create_pipeline_samplesheet_csv(
    rununit,
    run_id: str,
    base_path: str,
    output_file: str = "pipeline_samplesheet.csv"
):
    
    base_path = base_path + "/" +str(run_id_example)
    rununit_data = rununit[0]
    lanes = rununit_data.get("numberoflanes")
    print("lanes",lanes)
    # Build the rows
    rows = []
    for i in range(1,int(lanes)+1):
        rows.append([
            run_id,
            f"{base_path}/Samplesheet.csv",
            str(i),
            base_path
        ])
    
    # Write the CSV
    with open(output_file, mode="w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        # Write header
        writer.writerow(["id", "samplesheet", "lane", "flowcell"])
        # Write rows
        writer.writerows(rows)
    
    print(f"pipeline_samplesheet.csv written to: {output_file}")







def manipulate_date_format(original_str):
    # 1. Parse the string into a datetime object (matching the YYYY-mm-dd HH:MM:SS pattern).
    dt_obj = datetime.strptime(original_str, "%Y-%m-%d %H:%M:%S")

    # 2. Extract the pieces (month, day, year) without leading zeros.
    month = dt_obj.month        # e.g., 2
    day   = dt_obj.day          # e.g., 26
    year  = dt_obj.year         # e.g., 2025

    # 3. Build the new string "M/D/YYYY".
    new_str = f"{month}/{day}/{year}"

    return new_str


#-----------------------------------------------------------------------------#

# Call the functions


#Create Samplesheet.csv
create_samplesheet(app_data, samples, rununit, instrument_data, "Samplesheet.csv")


#Create pipeline_samplesheet.csv
run_id_example = "200611_A00789R_0071_BHHVCCDRXX"
base_path = "/APPLICATION"

create_pipeline_samplesheet_csv(
    rununit,
    run_id=run_id_example,
    base_path=base_path,
    output_file="pipeline_samplesheet.csv"
    )
