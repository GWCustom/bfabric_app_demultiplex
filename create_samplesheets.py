"""
The purpose of this file is to create the data required to run the Nextflow pipeline
for NovaSeq6000: ILMN_mRNA_Ligation_Prep. Three files are required:

- Samplesheet.csv
- pipeline_samplesheet.csv
- NFC_DMX.config

The Samplesheet.csv and pipeline_samplesheet.csv files are generated by the
`create_samplesheets` function, while the NFC_DMX.config file must be created
manually by the user.
"""

import sys
import csv
import os
from datetime import datetime

sys.path.append("../bfabric-web-apps")

import bfabric_web_apps
from bfabric_web_apps.objects.BfabricInterface import bfabric_interface
from sample_sheet import SampleSheet, Sample


def create_samplesheets(
    token_data,
    app_data,
    output_file_samplesheet="Samplesheet.csv",
    output_file_pipeline_samplesheet="pipeline_samplesheet.csv"
):
    """
    Create a NovaSeq6000 sample sheet (mRNA ligation prep) with format:
    [Header], [Reads], [Settings], [Data].
    This function will:
      1. Read metadata (samples, run, instrument, etc.) via bfabric_web_apps.
      2. Populate a SampleSheet object.
      3. Write the final Samplesheet.csv.
      4. Create a companion pipeline_samplesheet.csv for Nextflow use.
    """
    L = bfabric_web_apps.get_logger(token_data)
    wrapper = bfabric_interface.get_wrapper()

    # Read required metadata via the Bfabric API
    samples = L.logthis(
        api_call=wrapper.read,
        endpoint="sample",
        obj={"runid": token_data["entity_id_data"]},
        params=None,
        flush_logs=False
    )

    run = L.logthis(
        api_call=wrapper.read,
        endpoint="run",
        obj={"id": 1913},
        flush_logs=False
    )

    rununit = L.logthis(
        api_call=wrapper.read,
        endpoint="rununit",
        obj={"runid": token_data["entity_id_data"]},
        flush_logs=False
    )

    instrument_id = rununit[0]["instrument"]["id"]
    instrument_data = L.logthis(
        api_call=wrapper.read,
        endpoint="instrument",
        obj={"id": instrument_id},
        flush_logs=False
    )

    workflow = L.logthis(
        api_call=wrapper.read,
        endpoint="workflow",
        obj={"id": 1619},
        flush_logs=False
    )

    print("samples", samples)
    print("run", run)
    print("rununit", rununit)


    # Prepare some metadata
    rununit_data = rununit[0]
    instrument_data = instrument_data[0]

    # 1. Create a fresh SampleSheet object
    ss = SampleSheet()

    # 2. Populate the [Header] section
    ss.Header["IEMFileVersion"] = 5  # Hardcode
    ss.Header["Experiment Name"] = rununit_data.get("name")
    ss.Header["Date"] = manipulate_date_format(rununit_data.get("created"))
    ss.Header["Workflow"] = "GenerateFASTQ"  # Hardcode
    ss.Header["Application"] = app_data.get("name")
    ss.Header["Instrument Type"] = instrument_data.get("name")
    # ss.Header["Assay"] = "Nextera Flex for Enrichment"
    # ss.Header["Index Adapters"] = "IDT-ILMN Nextera DNA UD Indexes (384 Indexes)"

    # 3. Populate the [Reads] section
    ss.Reads = [76, 76]  # Hardcode

    # 4. Populate the [Settings] section
    ss.Settings["Adapter"] = "CTGTCTCTTATACACATCT"  # Hardcode

    # 5. Create rows in the [Data] section
    for record in samples:
        sample_dict = {
            "Sample_ID": record["id"],
            "Sample_Name": record["name"],
            "Sample_Plate": "",
            "Sample_Well": "",
            "Index_Plate": "",
            "Index_Plate_Well": "",
            "I7_Index_ID": record["multiplexiddmx"], # purely for final CSV documentation
            "index": record["multiplexiddmx"],
            "I5_Index_ID": record["multiplexid2dmx"], # purely for final CSV documentation
            "index2": record["multiplexid2dmx"],
            "Sample_Project": record["container"]["id"],
            "Description": ""
        }
        ss.add_sample(Sample(sample_dict))

    # 6. Write to CSV (Samplesheet.csv)
    with open(output_file_samplesheet, "w", newline="") as handle:
        ss.write(handle)
    print(f"Sample sheet written to {output_file_samplesheet}")

    # Create pipeline_samplesheet.csv
    create_pipeline_samplesheet_csv(
        output_file_samplesheet,
        run,
        rununit,
        output_file_pipeline_samplesheet
    )

    return list(samples)


def create_pipeline_samplesheet_csv(
    Samplesheet_name,
    run,
    rununit,
    output_file
):
    """
    Create the pipeline_samplesheet.csv for Nextflow usage.
    This CSV has 4 columns: id, samplesheet, lane, flowcell.
    """
    run = run[0]
    rununit_data = rununit[0]
    lanes = rununit_data.get("numberoflanes")
    print("lanes", lanes)

    # Extract the final directory name from the run's datafolder
    run_id = os.path.basename(run.get("datafolder"))

    # Build rows for each lane
    rows = []
    for i in range(1, int(lanes) + 1):
        rows.append([
            run_id,
            run.get("datafolder") + "/" + Samplesheet_name,
            str(i),
            run.get("datafolder")
        ])

    # Write the CSV
    with open(output_file, mode="w", newline="") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["id", "samplesheet", "lane", "flowcell"])
        writer.writerows(rows)

    print(f"pipeline_samplesheet.csv written to: {output_file}")


def manipulate_date_format(original_str):
    """
    Convert date strings from 'YYYY-mm-dd HH:MM:SS' to 'M/D/YYYY'
    (dropping leading zeros from the month/day).
    """
    dt_obj = datetime.strptime(original_str, "%Y-%m-%d %H:%M:%S")
    month = dt_obj.month
    day = dt_obj.day
    year = dt_obj.year
    return f"{month}/{day}/{year}"